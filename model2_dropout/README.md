layer - > 0===============<br>class_name = Dense<br>units = 256<br>activation = linear<br>layer - > 1===============<br>class_name = Activation<br>activation = relu<br>layer - > 2===============<br>class_name = Dropout<br>rate = 0.2<br>layer - > 3===============<br>class_name = Dense<br>units = 128<br>activation = linear<br>layer - > 4===============<br>class_name = Activation<br>activation = relu<br>layer - > 5===============<br>class_name = Dropout<br>rate = 0.2<br>layer - > 6===============<br>class_name = Dense<br>units = 64<br>activation = linear<br>layer - > 7===============<br>class_name = Activation<br>activation = relu<br>layer - > 8===============<br>class_name = Dense<br>units = 32<br>activation = linear<br>layer - > 9===============<br>class_name = Activation<br>activation = relu<br>layer - > 10===============<br>class_name = Dense<br>units = 11<br>activation = linear<br>layer - > 11===============<br>class_name = Activation<br>activation = softmax<br><br>train epochs = 300<br>train batch = 128<br>test batch = 32<br>train time = 14595.877679347992<br>loss = 1.95676832638<br><br><br>accuracy = 0.31286020089