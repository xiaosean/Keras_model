layer - > 0===============<br>class_name = Dense<br>units = 1024<br>activation = linear<br>layer - > 1===============<br>class_name = Dense<br>units = 512<br>activation = linear<br>layer - > 2===============<br>class_name = Activation<br>activation = relu<br>layer - > 3===============<br>class_name = Dense<br>units = 256<br>activation = linear<br>layer - > 4===============<br>class_name = Activation<br>activation = relu<br>layer - > 5===============<br>class_name = Dense<br>units = 128<br>activation = linear<br>layer - > 6===============<br>class_name = Activation<br>activation = relu<br>layer - > 7===============<br>class_name = Dense<br>units = 64<br>activation = linear<br>layer - > 8===============<br>class_name = Activation<br>activation = relu<br>layer - > 9===============<br>class_name = Dense<br>units = 32<br>activation = linear<br>layer - > 10===============<br>class_name = Activation<br>activation = relu<br>layer - > 11===============<br>class_name = Dense<br>units = 11<br>activation = linear<br>layer - > 12===============<br>class_name = Activation<br>activation = softmax<br><br>train epochs = 300<br>train batch = 128<br>test batch = 32<br>train time = 16732.53845667839<br>loss = 2.25451076395<br><br><br>accuracy = 0.264520909579