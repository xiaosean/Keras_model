layer - > 0===============<br>class_name = Dense<br>units = 256<br>activation = linear<br>layer - > 1===============<br>class_name = Activation<br>activation = relu<br>layer - > 2===============<br>class_name = Dense<br>units = 128<br>activation = linear<br>layer - > 3===============<br>class_name = Activation<br>activation = relu<br>layer - > 4===============<br>class_name = Dense<br>units = 64<br>activation = linear<br>layer - > 5===============<br>class_name = Activation<br>activation = relu<br>layer - > 6===============<br>class_name = Dense<br>units = 32<br>activation = linear<br>layer - > 7===============<br>class_name = Activation<br>activation = relu<br>layer - > 8===============<br>class_name = Dense<br>units = 11<br>activation = linear<br>layer - > 9===============<br>class_name = Activation<br>activation = softmax<br><br>train epochs = 300<br>train batch = 128<br>test batch = 32<br>train time = 1382.2875151634216<br>loss = 3.27001998035<br><br><br>accuracy = 0.235306615676